---
title: "timeOmics"
author: 
- name:  "Antoine Bodein"
  affiliation: "CHU de Québec Research Center, Université Laval, Molecular Medicine department, Québec, QC, Canada"
  email: "antoine.bodein.1@ulaval.ca"
package: timeOmics
output: 
  BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
---

# Introduction

We propose a generic data-driven framework to integrate different types of longitudinal data measured on the same biological specimens with microbial communities data, and select key temporal features with strong associations within the same sample group. The framework ranges from filtering and modelling, to integration using smoothing splines and multivariate dimension reduction methods to address some of the analytical challenges of microbiome-derived data, as illustrated in the figure below. We present our framework on different types of multi-omics case studies in bioreactor experiments as well as human studies.

For more details please check: 

*Bodein A, Chapleur O, Droit A and Lê Cao K-A (2019) A Generic Multivariate Framework for the Integration of Microbiome Longitudinal Studies With Other Data Types. Front. Genet. 10:963. doi: 10.3389/fgene.2019.00963*

# Start

## Installation

```{r, echo =  F}
knitr::opts_chunk$set(eval = TRUE, 
                      echo = TRUE,
                      fig.align = "center",
                      warning = FALSE,
                      message = FALSE)
```


```r
install.packages("devtools")
# then load
library(devtools)
install_github("abodein/timeOmics_BioC")
```

## Load the package

```{r, message=F, warning=F}
library(timeOmics)
```

# Data preprocessing

The user should use his favorite method of standardization. 
However, we focussed on microbiome data analysis and we propose a suitable pre-processing steps for microbial data (16S or WGS).

1. Low Count Removal
2. Centered Log Ratio

# Time Modelling

We rely on a *Linear Mixed Model Splines* framework (package `LMMS`) to model features expression as a function of time.

LMMS framework tests 4 different models and assigns one of the following models to each feature:

* 0 = linear model, 
* 1 = linear mixed effect model spline (LMMS) with defined basis, 
* 2 = LMMS taking subject-specific random intercept, 
* 3 = LMMS with subject specific intercept and slope.

LMMS will take into account all inter-individual variability and summarize each feature into a unique time profile.

## Required data

Let's run `lmms` on simulated dataset.


```{r}
library(lmms)
library(tidyverse) # this one will be useful for the rest

data("timeOmics.simdata")
sim.data <- timeOmics.simdata$sim
sim.data[1:15,1:6]
```

LMMS requires a dataframe with features in columns and samples in rows.
Here my rownames are represented like this: **ID_Time**

For more information about LMMS modelling parameters, please check (link lmms)

```{r}
time <- rownames(sim.data) %>% str_remove("._") %>% as.numeric()
LMMS.output <- lmms::lmmSpline(data = sim.data, time = time,
                        sampleID = rownames(sim.data), deri = FALSE,
                        basis = "p-spline", numCores = 4, timePredict = 1:9, 
                        keepModels = T)
data <- t(LMMS.output@predSpline)
```

## LMMS output 

LMMS reduces the dimension of the data.
The produced table contains features in columns and now, times in lines.

Let's plot the modelled profiles.

```{r}
# gather data
data.gathered <- data %>% as.data.frame() %>% 
  rownames_to_column("time") %>%
  mutate(time = as.numeric(time)) %>%
  gather(feature, value, -time)

# plot profiles
ggplot(data.gathered, aes(x = time, y = value, color = feature)) + geom_line() +
  theme_bw() + ggtitle("LMMS profiles") + ylab("Features expression") +
  xlab("Time")
```

## Profile filtering

Straight line modelling can occur when the inter-individual variation is too high.
To remove the noisy profiles, we first use the Breusch-Pagan test, which tests the homo-sedasticity of the residues.
We then add a filter on the mean squared error to reduce the dispersion of the residues around the line.

```{r}
# not yet implemented
```

# (s)PCA longitudinal clustering

From the modelled data, we use a PCA to cluster features with a samilar expression profile over time.

PCA is un unsupervised reduction dimension technique which use uncorrelated 
intrumental variable (a.k.a principal components) to summarize as much information 
(*variance*) as possible from the data.

```{r}
data("timeOmics.simdata")
data <- timeOmics.simdata$modelled
```

## clustering

Text about clustering and how it is done in our case. 

* loading scores
* component
* contribution
* silhouette coefficient

Optimize the number of component
First, run a PCA and optimise 

```{r}
# run pca
pca.res <- pca(X = data, ncomp = 5)

pca.ncomp <- getNcomp(pca.res, max.ncomp = 5)
plot(pca.ncomp)
```

Thanks to the previous plot, highest silhouette coefficient is achieved when `ncomp = 2`.

```{r}
pca.res <- pca(X = data, ncomp = 2)
```

All information about the cluster is displayed here.

```{r}
# extract cluster
pca.cluster <- getCluster(pca.res)
head(pca.cluster)
```

### Plot PCA clusters

```{r}
plotVar(pca.res)
plotLong(pca.res)
```


## sparse PCA

The previous clustering used all features. Sometimes we are interested in a cluster signature.
We then use the sparse PCA to extract this key signature.

To find the right number of features to keep per component and thus per cluster, we evaluate the silhouette for a list of selected features on each component.

We do not recommend using a parameter that is too small since it will tend to pull the silhouette coefficient upwards and bias the interpretation regarding the number of features to be selected.

We will then follow the evolution of the silhouette coefficient of each cluster (component and contribution). 
The main idea here is to detect a significant decrease in the evolution of the silhouette for each component.
In other words, if we add 1 OTU, will the cluster be distorted?

```{r}
data <- timeOmics.simdata$modelled
tune.spca.res <- tuneCluster.spca(X = data, ncomp = 2, test.keepX = c(2:10))
plot(tune.spca.res)
plot(tune.spca.res, comp = 2)
```

From the following graphs, silhouette coefficient tends to decrease at 8 for component 1, and 7 at component 2.

```{r}
spca.res <- spca(X = data, ncomp = 2, keepX = c(8,7), scale = F)
plotLong(spca.res)
```


# (s)PLS longitudinal clustering

```{r}
X <- data
Y <- data[,sample(1:20, size = 8)]
colnames(Y) <- paste0("Y", 1:ncol(Y))

pls.res <- pls(X,Y, ncomp = 2, scale = T)

getCluster(pls.res) %>% head

plotLong(pls.res)

tune.spls <- tuneCluster.spls(X, Y, ncomp = 2, test.keepX = c(5,10,15,20), test.keepY <- c(2,4,6))
```

need a graph for keepX/keepY

# Multi-block (s)PLS longitudinal clustering

```{r}
X <- data
Y <- data[,sample(1:20, size = 8)]
colnames(Y) <- paste0("Y", 1:ncol(Y))

Z <- data[,sample(1:20, size = 12)]
colnames(Z) <- paste0("Z", 1:ncol(Z))

block.pls.res <- block.pls(X = list("X"=X,"Z"=Z), Y, ncomp = 2, scale = T)

block.pls.cluster <- getCluster(block.pls.res)
head(block.pls.cluster)

plotLong(block.pls.res)
```

<!--need a graph for block.spls -->

# Post-hoc evaluation

We compute the proportionality distance between features of the same cluster.
By keeping the features close to each other, it guarantees the authenticity of the interactions.
We also use a Wilcoxon U-test to compare the within cluster median compared to the entire background set.

```{r}
# example fro multiblock analysis
res <- proportionality(block.pls.res)
# distance between pairs of features
res$propr.distance
# u-test pvalue by clusters
res$pvalue
plot(res)
```

Explication
